{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jieba\n",
    "import pandas\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "myseed = 12345\n",
    "torch.manual_seed(myseed)\n",
    "torch.random.manual_seed(myseed)\n",
    "random.seed(0)\n",
    "np.random.seed(myseed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.cuda.manual_seed_all(myseed)\n",
    "torch.autograd.set_detect_anomaly(True)  # 可在NaN 出现时报错，定位错误代码。正向传播时：开启自动求导的异常侦测\n",
    "# 反向传播时：在求导时开启侦测\n",
    "#with torch.autograd.detect_anomaly():\n",
    "#    loss.backward()\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "with open(\"./dataset/HPD/cn_train_set.json\", 'r') as file:\n",
    "    train_file = json.load(file)\n",
    "with open(\"./dataset/HPD/cn_test_set.json\", 'r') as file:\n",
    "    test_file = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载停用词表\n",
    "with open('dataset/停用词表.txt', encoding='utf-8') as f:\n",
    "    con = f.readlines()\n",
    "    stop_words = set()\n",
    "    for i in con:\n",
    "        i = i.replace(\"\\n\", \"\")\n",
    "        stop_words.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对对话词元化\n",
    "i = 0\n",
    "train_words = []\n",
    "for segment in train_file:\n",
    "    for line in train_file[segment][\"对话历史\"]:\n",
    "        line_words = []\n",
    "        line = line.split(\"：\")[1]\n",
    "        print(line)\n",
    "        tokens = jieba.cut(line, cut_all=False)\n",
    "        for token in tokens:\n",
    "            # 去停用词\n",
    "            if token not in stop_words:\n",
    "                line_words.append(token)\n",
    "        train_words.append(line_words)\n",
    "\n",
    "i = 0\n",
    "test_words = []\n",
    "for segment in test_file:\n",
    "    for line in test_file[segment][\"对话历史\"]:\n",
    "        line_words = []\n",
    "        line = line.split(\"：\")[1]\n",
    "        tokens = jieba.cut(line, cut_all=False)\n",
    "        for token in tokens:\n",
    "            # 去停用词\n",
    "            if token not in stop_words:\n",
    "                line_words.append(token)\n",
    "        test_words.append(line_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建词表\n",
    "class Vocab:\n",
    "    def __init__(self, words, min_freq = 0) -> None:\n",
    "        if tokens is None:\n",
    "            return\n",
    "\n",
    "        # 计算频率\n",
    "        self.words_num = {}\n",
    "        for line in words:\n",
    "            for word in line:\n",
    "                if word not in self.words_num:\n",
    "                    self.words_num[word] = 1\n",
    "                else:\n",
    "                    self.words_num[word] += 1\n",
    "\n",
    "        # 删掉出现次数较少的词\n",
    "        self.words = words\n",
    "        for word in list(self.words_num.keys()):\n",
    "            if self.words_num[word] <= min_freq:\n",
    "                del self.words_num[word]\n",
    "        for i in range(len(self.words)):\n",
    "            for j in range(len(self.words[i])):\n",
    "                if self.words[i][j] not in self.words_num:\n",
    "                    self.words[i][j] = \"<unk>\"\n",
    "        self.words_num[\"<unk>\"] = 0\n",
    "        self.len = len(self.words_num)\n",
    "\n",
    "        # 按照出现次数排序\n",
    "        self.words_num = sorted(self.words_num.items(),  key=lambda d: d[1], reverse=True)\n",
    "\n",
    "        # index 与 word\n",
    "        i = 0\n",
    "        self.idx_word = {}\n",
    "        for key in self.words_num:\n",
    "            self.idx_word[key[0]] = i\n",
    "            i += 1\n",
    "    \n",
    "    def __word__(self, index):\n",
    "        return self.words_num[index]\n",
    "\n",
    "    def __words__(self):\n",
    "        return self.words\n",
    "    \n",
    "    def __words_num__(self):\n",
    "        return self.words_num\n",
    "    \n",
    "    def __index__(self, str):\n",
    "        return self.idx_word[str]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "train_table = Vocab(train_words, 5)\n",
    "test_table= Vocab(test_words, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "def seq_data_iter_random(vtab, corpus, batch_size, num_steps):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    for i in range(len(corpus)):\n",
    "        x = []\n",
    "        y = []\n",
    "        for j in range(batch_size):\n",
    "            start_index = np.random.randint(num_steps)\n",
    "            # x\n",
    "            if i % 2:\n",
    "                cur_x = []\n",
    "                if len(corpus[i]) < (start_index + num_steps):\n",
    "                    for k in range(start_index, len(corpus[i])):\n",
    "                        cur_x.append(vtab.__index__(corpus[i][k]))\n",
    "                    n = 0\n",
    "                    while(n < num_steps - len(cur_x)):\n",
    "                        cur_x.append(0)\n",
    "                else:\n",
    "                    for k in range(start_index, start_index + num_steps):\n",
    "                        cur_x.append(vtab.__index__(corpus[i][k]))\n",
    "               \n",
    "                x.append(cur_x)\n",
    "            # y\n",
    "            else:\n",
    "                cur_y = []\n",
    "                if len(corpus[i]) < (start_index + num_steps):\n",
    "                    for k in range(start_index, len(corpus[i])):\n",
    "                        cur_y.append(vtab.__index__(corpus[i][k]))\n",
    "                    n = 0\n",
    "                    while(n < num_steps - len(cur_y)):\n",
    "                        cur_y.append(0)\n",
    "                else:\n",
    "                    for k in range(start_index, start_index + num_steps):\n",
    "                        cur_y.append(vtab.__index__(corpus[i][k]))\n",
    "                y.append(cur_y)\n",
    "        if i % 2:\n",
    "            batch_x.append(x)\n",
    "        else:\n",
    "            batch_y.append(y)\n",
    "    return batch_x, batch_y\n",
    "\n",
    "train_batch_x, train_batch_y = seq_data_iter_random(train_table, train_table.__words__(), 2, 5)\n",
    "test_batch_x, test_batch_y = seq_data_iter_random(test_table, test_table.__words__(), 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 GPU\n",
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def try_all_gpus():  #@save\n",
    "    \"\"\"返回所有可用的GPU，如果没有GPU，则返回[cpu(),]\"\"\"\n",
    "    devices = [torch.device(f'cuda:{i}')\n",
    "             for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转为张量\n",
    "train_x = torch.tensor(data=train_batch_x, device=try_gpu(), requires_grad=True, dtype=torch.float)\n",
    "#train_x = torch.nn.functional.one_hot(train_x.to(torch.int64), num_classes=train_table.__len__())\n",
    "train_y = torch.tensor(data=train_batch_y, device=try_gpu(), dtype=torch.float)\n",
    "test_x = torch.tensor(data=test_batch_x, device=try_gpu(), requires_grad=True, dtype=torch.float)\n",
    "test_y = torch.tensor(data=test_batch_y, device=try_gpu(), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(torch.nn.Module):\n",
    "    \"\"\"循环神经网络模型\n",
    "\n",
    "    Defined in :numref:`sec_rnn-concise`\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1\n",
    "        self.linear = torch.nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "       \n",
    "    def forward(self, inputs, state):\n",
    "        X = torch.nn.functional.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)\n",
    "        # 它的输出形状是(时间步数*批量大小,词表大小)。\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, torch.nn.LSTM):\n",
    "            # nn.GRU以张量作为隐状态\n",
    "            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                device=device)\n",
    "        else:\n",
    "            # nn.LSTM以元组作为隐状态\n",
    "            return (torch.zeros((\n",
    "                self.num_directions * self.rnn.num_layers,\n",
    "                batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((\n",
    "                        self.num_directions * self.rnn.num_layers,\n",
    "                        batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_table.__len__())\n",
    "model = GRU(train_table.__len__(), 5, 256, 10)\n",
    "model = model.to(device=try_gpu())\n",
    "\n",
    "# 定义优化器和损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "epochs= 1000\n",
    "\n",
    "Loss_data = {\n",
    "    \"train\": [],\n",
    "    \"dev\": []\n",
    "}\n",
    "\n",
    "train_len = len(train_x)\n",
    "test_len = len(test_x)\n",
    "\n",
    "# 训练\n",
    "for i in tqdm.tqdm(range(epochs)):\n",
    "    Loss = 0\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    y_hat = model(train_x)\n",
    "    #state = state_new\n",
    "    loss = criterion(y_hat, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    running_train_loss += loss.item()\n",
    "    Loss_data[\"train\"].append(loss.item())\n",
    "\n",
    "    # 训练损失\n",
    "    epoch_train_loss = running_train_loss / train_len\n",
    "    perplexity_train = np.exp(running_train_loss / train_len)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    # 测试损失\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_x)\n",
    "        outputs = outputs.to(torch.float)\n",
    "        outputs = outputs.requires_grad_(True)\n",
    "        pred = outputs.reshape(test_x.shape[0], 2, 5)\n",
    "        loss = criterion(pred, test_y)\n",
    "        running_test_loss += loss.item()\n",
    "        Loss_data[\"dev\"].append(loss.item())\n",
    "    epoch_test_loss = running_test_loss / test_len\n",
    "    Loss_data[\"dev\"].append(epoch_test_loss)\n",
    "    perplexity_test = np.exp(running_test_loss / test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(loss_record, title=''):\n",
    "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
    "    x_1 = len(loss_record['train'])\n",
    "    x_2 = len(loss_record[\"dev\"])\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(range(x_1), loss_record['train'], c='tab:red', label='train')\n",
    "    plt.plot(range(x_2), loss_record['dev'], c='tab:cyan', label='dev')\n",
    "    plt.ylim(0.0, )\n",
    "    plt.xlabel('Training steps')\n",
    "    plt.ylabel('MSE loss')\n",
    "    plt.title('Learning curve of {}'.format(title))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curve(Loss_data, \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"perplexity train: \", perplexity_train)\n",
    "print(\"perplexity test: \", perplexity_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
