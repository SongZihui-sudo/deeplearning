{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jieba\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "with open(\"./dataset/HPD/cn_train_set.json\", 'r') as file:\n",
    "    train_file = json.load(file)\n",
    "with open(\"./dataset/HPD/cn_test_set.json\", 'r') as file:\n",
    "    test_file = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载停用词表\n",
    "with open('dataset/停用词表.txt', encoding='utf-8') as f:\n",
    "    con = f.readlines()\n",
    "    stop_words = set()\n",
    "    for i in con:\n",
    "        i = i.replace(\"\\n\", \"\")\n",
    "        stop_words.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对对话词元化\n",
    "train_words = {}\n",
    "for segment in train_file:\n",
    "    for line in train_file[segment][\"对话历史\"]:\n",
    "        tokens = jieba.cut(line, cut_all=False)\n",
    "        for token in tokens:\n",
    "            # 去停用词\n",
    "            if token not in stop_words and len(token) > 1:\n",
    "                if token in train_words:\n",
    "                    train_words[token] += 1\n",
    "                else:\n",
    "                    train_words[token] = 1\n",
    "test_words = {}\n",
    "for segment in test_file:\n",
    "    for line in test_file[segment][\"对话历史\"]:\n",
    "        tokens = jieba.cut(line, cut_all=False)\n",
    "        for token in tokens:\n",
    "            # 去停用词\n",
    "            if token not in stop_words and len(token) > 1:\n",
    "                if token in test_words:\n",
    "                    test_words[token] += 1\n",
    "                else:\n",
    "                    test_words[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pandas.DataFrame()\n",
    "train_df[\"Word\"] = train_words.keys()\n",
    "train_df[\"Num\"] = train_words.values()\n",
    "test_df = pandas.DataFrame()\n",
    "test_df[\"Word\"] = test_words.keys()\n",
    "test_df[\"Num\"] = test_words.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建词表\n",
    "class Vocab:\n",
    "    def __init__(self, tokens=None, min_freq = 0) -> None:\n",
    "        if tokens is None:\n",
    "            return\n",
    "        # 计算频率\n",
    "        freq = (tokens[\"Num\"] / tokens[\"Num\"].count()) * 100\n",
    "        tokens[\"Freq\"] = freq\n",
    "        print(tokens[\"Freq\"].describe())\n",
    "        \n",
    "        if not min_freq:\n",
    "            min_freq = tokens[\"Freq\"].min()\n",
    "\n",
    "        # 删除频率太低的词\n",
    "        tokens.drop(tokens[(tokens.Freq <= min_freq)].index, inplace=True)\n",
    "        # 按频率排序\n",
    "        self.tokens_sort = tokens.sort_values(by='Freq', ascending=False)\n",
    "        self.tokens_sort.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tokens_sort.count()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index < 0 or index > len(self.tokens_sort):\n",
    "            return\n",
    "        return self.tokens_sort.iloc[index]\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return 0\n",
    "    \n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self.tokens_sort[\"Freq\"]\n",
    "\n",
    "train_set = Vocab(train_df)\n",
    "test_set= Vocab(test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
